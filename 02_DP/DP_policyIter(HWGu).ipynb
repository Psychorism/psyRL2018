{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "## 1. Dynamic Programming & Grid World\n",
    "\n",
    "### 1.1. Recap : MDP\n",
    "\n",
    "MDP is a **discrete time stochastic process**. \n",
    "\n",
    "An **MDP** is a 5-tuple $(\\mathcal{S},\\mathcal{A},\\mathcal{P}_a,\\mathcal{R}_a,\\gamma)$.\n",
    "\n",
    "**That is**, $(S_1,A_1,\\mathcal{P}^{a_1}_{s_1 s'},\\mathcal{R}^{a_1}_{s_1},\\gamma)$, $(S_2,A_2,\\mathcal{P}^{a_2}_{s_2 s'},\\mathcal{R}^{a_2}_{s_2},\\gamma)$, $(S_3,A_3,\\mathcal{P}^{a_3}_{s_3 s'},\\mathcal{R}^{a_3}_{s_3},\\gamma)$, $\\cdots$ until the **stopping time $\\tau$**.\n",
    "\n",
    "1. **State** ($S \\in \\mathcal{S}$)\n",
    "\n",
    "2. **Action** ($A \\in \\mathcal{A}$)\n",
    "\n",
    "    - $A_s$ is the finite set of actions available from state\n",
    "\n",
    "3. **(State) Transition Probability** ($\\mathcal{P}_a$)\n",
    "\n",
    "    - $\\mathcal{P}^a_{ss'} = Pr[S_{t+1} = s' \\mid S_t = s, A_t = a]$ \n",
    "    \n",
    "    - the probability that action $a$ in state $s$ at time $t$ will lead to state $s'$ at time $(t+1)$.\n",
    "\n",
    "4. **Reward Function** ($\\mathcal{R}^a_s$)\n",
    "\n",
    "    - $\\mathcal{R}^a_s = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]$\n",
    "\n",
    "    - the expected immediate reward received after transitioning from state $s$ to state $s'$, due to action $a$\n",
    "    \n",
    "5. **Discount Factor** ($\\gamma$)\n",
    "\n",
    "    - it represents the difference in importance between future rewards and present rewards.\n",
    "\n",
    "    - used to avoid the problem of infinite return ($G_t = R_{t+1} + \\gamma R_{t+1} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$)\n",
    "    \n",
    "### 1.2. A Sequential Decision Making Problem : MDP vs. Reality\n",
    "\n",
    "**Example of \"Street Dancing\"**\n",
    "\n",
    "1. **State**($S_t$) : a specific **posture** at time point $t$\n",
    "\n",
    "2. **Action**($A_t$) : a specific **movement** at time point $t$\n",
    "\n",
    "3. **Transition Probability**($P^{a_t}_{s_t s'}$) : the probability that **movement** $a_t$ from the **posture** $s_t$ will lead to the next **posture** $s'$.\n",
    "\n",
    "4. **Reward Function**($R^{a_t}_{s_t}$) : the amount of **applause** from the crowd received from from the **posture** $s_t$ to $s'$, due to the **movement** $a_t$.\n",
    "\n",
    "5. **Discount Factor** ($\\gamma$) : the difference in importance between future **applause** and present **applause**.\n",
    "\n",
    "(Note that **1,2,4** are the **random variables**)\n",
    "\n",
    "|MDP|Reality\n",
    "-|-|-\n",
    "$\\mathcal{S}$|known|known\n",
    "$\\mathcal{A}$|known|known\n",
    "$\\mathcal{P}$|**known beforehand**|**unknown**\n",
    "$\\mathcal{R}$|**known beforehand**|**unknown until observed**\n",
    "$\\gamma$|known|known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Dynamic Programming (DP)\n",
    "\n",
    "**What is DP**?\n",
    "\n",
    "+ A problem-solving paradigm, where the problem is **split into several sub-problems** and the **past solutions are reused**.\n",
    "\n",
    "+ If sub-problems can be **nested recursively** inside larger problems, so that dynamic programming methods are applicable, then **there is a relation between the value of the larger problem and the values of the sub-problems**.\n",
    "\n",
    "**Why DP**?\n",
    "\n",
    "+ In order to effectively find the **target function**.\n",
    "\n",
    "+ To solve the **recurrence formula** by dividing the problem into smaller processes\n",
    "\n",
    "$$\n",
    "v_0(s) \\rightarrow v_1(s) \\rightarrow \\cdots \\rightarrow v_k(s) \\rightarrow \\cdots \\rightarrow v_{\\pi}(s)\n",
    "$$\n",
    "\n",
    "**How to find the value function?**\n",
    "\n",
    "+ **Value function** is the function of states that estimated how good it is for the agent to be in a given state.\n",
    "\n",
    "+ The notion of \"how good\" is defined in terms of **expected return**.\n",
    "\n",
    "+ Thus, we can adopt a **quick-and-dirty approach** using simulations.\n",
    "\n",
    "### 1.4. Grid World Example (Quick-and-Dirty)\n",
    "\n",
    "Let us assume a $3 \\times 3$ grid world.\n",
    "\n",
    "For simplicity, we can assume the following.\n",
    "\n",
    "$$\\mathcal{P}_a = 1,\\mathcal{R}_a = 0, \\text{ (in terminal, } \\mathcal{R}_a = 1) ,\\gamma=0.7$$\n",
    "\n",
    "And we have **random policy**, which can be defined as\n",
    "\n",
    "$$\n",
    "\\pi(a \\mid s) = 0.25 \\ \\ \\forall a\n",
    "$$\n",
    "\n",
    "Without DP, we still be able to **approximate $v$ function**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0 \tS=(0, 0) \tA=up \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=1 \tS=(0, 0) \tA=east \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=2 \tS=(0, 1) \tA=west \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=3 \tS=(0, 0) \tA=west \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=4 \tS=(0, 0) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=5 \tS=(1, 0) \tA=up \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=6 \tS=(0, 0) \tA=west \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=7 \tS=(0, 0) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=8 \tS=(1, 0) \tA=west \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=9 \tS=(1, 0) \tA=west \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=10 \tS=(1, 0) \tA=east \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=11 \tS=(1, 1) \tA=east \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=12 \tS=(1, 2) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=13 \tS=(2, 2) \t \t \tR=1.0 \tgamma=0.7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1066e8128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<DP.randomPolicy at 0x1066e8048>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DP import *\n",
    "\n",
    "env = Env()\n",
    "randomPolicy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03170211  0.06388527  0.11233541]\n",
      " [ 0.06651981  0.15223633  0.28577732]\n",
      " [ 0.12016512  0.28573636  0.7       ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DP.Simulator at 0x1066c9e80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Simulator(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamic Programming 1 : Policy Iteration\n",
    "\n",
    "### 2.1. Flow of RL algorithms\n",
    "\n",
    "**Equation**|**Algorithm**|\n",
    "-|-\n",
    "**Bellman Expectation Equation** | Policy iteration\n",
    "**Bellman Optimality Equation** | Value iteration\n",
    "\n",
    "\n",
    "### 2.2. Policy Iteration\n",
    "\n",
    "+ **Policy** $\\rightarrow$ **Policy Evaluation** $\\rightarrow$ **Policy Improvement**\n",
    "\n",
    "+ This iteration can make the policy to converge to the **optimal policy**.\n",
    "\n",
    "### 2.3. Policy Evaluation\n",
    "\n",
    "We can redefine **Bellman Expectation Equation** in a **recursive form**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[G_t \\mid S_t =s ] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t =s] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t =s ] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t =s ] \\\\[15pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be further modified so that it can be **calculated**. Note **how $v$ is dependent upon $\\pi$**.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t =s ] \\\\[15pt]\n",
    "&= \\sum_{a \\in A} \\pi (a \\mid s) (R_{t+1} + \\gamma v_{\\pi}(s') ) ) \\\\[15pt]\n",
    "v_{k+1}(s) &= \\sum_{a \\in A} \\pi (a \\mid s) (R^{a}_{s} + \\gamma v_{k}(s') ) ) \\\\[15pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 2.4. Grid World Example (DP)\n",
    "\n",
    "Using the above formula, we can **easily calculate the $v$ function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00133984  0.00765625  0.        ]\n",
      " [ 0.          0.04375     0.30274609]\n",
      " [ 0.04375     0.25        0.66830254]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DP.Bellman at 0x1066e8080>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bellman(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Policy Improvement\n",
    "\n",
    "How can we improve our policy based on the **policy evaluation**?\n",
    "\n",
    "We introduce **Greedy Policy Improvement**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi} (s,a) &= R^{a}_{s} + \\gamma v_{\\pi} (s') \\\\[15pt]\n",
    "\\pi'(s) &= argmax_{a \\in A} q_{\\pi} (s, a)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
