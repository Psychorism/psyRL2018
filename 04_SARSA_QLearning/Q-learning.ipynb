{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Off-Policy control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DILEMMA** (on-policy control) : agents need to learn about the *optimal* policy while behaving according to an *exploratory* policy\n",
    "\n",
    "Off-Policy control separates target policy and behavior policy\n",
    "* Evaluate target policy $\\pi(a|s)$ to compute $V_{\\pi}(s)$ or $q_{\\pi}(s,a)$ while following behavior policy $\\mu(a |s)$\n",
    "* Learn from observing other agents\n",
    "* Re-use exprerience generated from old policies\n",
    "* Learn about optimal policy while following exploratory policy\n",
    "* Learn about *multiple* policies while following *one* policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimating properties of a particular distribution, while only having *samples generated from a different distribution* than the distribution of interest\n",
    "\n",
    "$$E_{X \\sim P}[f(X)]  = \\sum P(X)f(X)   = \\sum Q(X) \\frac{P(X)}{Q(X)} f(X)   = E_{X \\sim Q} \\left[ \\frac{P(X)}{Q(X)}f(X) \\right]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1. Importance Sampling for Off-Policy MC**\n",
    "\n",
    "* Use returns generated from $\\mu$ to evaluate $\\pi$\n",
    "* Multiply importance sampling corrections along whole episode\n",
    "$$ G_t^{\\pi / \\mu} = \\frac{\\pi(A_t|S_t)}{\\mu(A_t|S_t)} \\frac{\\pi(A_{t+1}|S_{t+1})}{\\mu(A_{t+1}|S_{t+1})} \\cdots \\frac{\\pi(A_T|S_T)}{\\mu(A_T|S_T)}G_t$$\n",
    "* Update value towards corrected return\n",
    "$$V(S_t) \\leftarrow V(S_t)+\\alpha \\left( G_{t}^{\\pi / \\mu} - V(S_t) \\right) $$\n",
    "* Importance sampling can dramatically increases variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2. Importance sampling for Off-Policy TD**\n",
    "* Use TD targets generated from $\\mu$ to evaluate $\\pi$\n",
    "* Weight TD target $R+\\gamma V(S') $ by importance sampling\n",
    "* Only need a single importance sampling correction\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha \\left( \\frac{\\pi(A_t|S_t)}{\\mu (A_t|S_t)}(R_{t+1}+\\gamma V(S_{t+1}))-V(S_t) \\right)$$\n",
    "* Much lower variance than Monte-Carlo importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **$\\epsilon$-greedy (behavior policy) & greedy (target policy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target policy $\\pi(S_{t+1})=\\underset{a'}{argmax}Q(S_{t+1},a')$\n",
    "\n",
    "$\\therefore$ target simplifies:\n",
    "\n",
    "$$ R_{t+1}+\\gamma Q(S_{t+1},A')   = R_{t+1}+\\gamma Q(S_{t+1},\\underset{a'}{argmax}Q(S_{t+1},a')) \\\\  = R_{t+1} +\\underset{a'}{max}\\gamma Q(S_{t+1},a')  $$\n",
    "\n",
    "updating\n",
    "\n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+\\alpha(R_{t+1}+\\gamma maxQ(S_{t+1},a')-Q(S_t,A_t))$$\n",
    "\n",
    "converges to the optimal action-value function : $Q(s,a) \\rightarrow q_{*}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "Initialize $Q(s,a), \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(S)$, arbitrarily, and $Q(terminal-state)=0$\n",
    "\n",
    "Repeat (for each episode):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Initialize $S$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Repeat (for each step of episode):\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Choose $A$ from $S$ using policy derived from $Q$ &nbsp;&nbsp; ** $\\ast$ $\\epsilon$ - greedy **\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Take action $A$, observe $R$,$S'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Q(S,A) \\leftarrow Q(S,A)+\\alpha \\left[ R+\\gamma max_aQ(S',a)-Q(S,A) \\right] $  &nbsp;&nbsp; ** $\\ast$ greedy **\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (**cf.SARSA** : Choose $A'$ from $S'$ using policy derived from $Q$  &nbsp;&nbsp;  ** $\\ast$ $\\epsilon$ - greedy ** \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(S,A) \\leftarrow Q(S,A) + \\alpha \\left[ R+\\gamma Q(S',A')-Q(S,A) \\right] $ )\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S \\leftarrow S'$;\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; until $S$ is terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning and SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gridworld example : Cliff Walking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
