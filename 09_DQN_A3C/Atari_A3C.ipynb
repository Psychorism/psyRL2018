{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C application to Atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (Review) A2C : Actor-critic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/1024/1*-GfRVLWhcuSYhG25rN0IbA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **actor** learns policies and the **critic** learns about whatever policy is currently being followed by the actor in order to **criticize** the actor's action choices.\n",
    "\n",
    "In only the policy network(**REINFORCE**), we have\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} \\approx \\theta_t + \\alpha [ \\nabla_\\theta \\log \\pi_\\theta (a | s) G_t]\n",
    "$$\n",
    "\n",
    "In both the policy and value network(**A2C**), we have\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} \\approx \\theta_t + \\alpha [ \\nabla_\\theta \\log \\pi_\\theta (a | s) Q_w(s,a)]\n",
    "$$\n",
    "\n",
    "In **A2C**, we have high variance of loss, because\n",
    "\n",
    "$$\n",
    "Loss = cross \\ \\ entropy \\ \\ (policy \\ \\ network) \\ \\ \\times \\ \\ Q-fun \\ \\ (value \\ \\ network)\n",
    "$$\n",
    "\n",
    "Thus, we adopt the **baseline** (which is independent of actions), and use **value function** as the baseline. Value function can also be approximated using a new variable, $v$. We define **advantage function**,\n",
    "\n",
    "$$\n",
    "A(S_t, A_t) = Q_w (S_t, A_t) - V_v (S_t)\n",
    "$$\n",
    "\n",
    "Approximating only the value function, \n",
    "\n",
    "$$\n",
    "\\delta_v = R_{t+1} + \\gamma V_v (S_{t+1}) - V_v (S_t)\n",
    "$$\n",
    "\n",
    "thus we have the **updating function of actor-critic** using the **advantage function**,\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} \\approx \\theta_t + \\alpha [\\nabla_\\theta \\log \\pi_\\theta (a|s) \\delta_v ]\n",
    "$$\n",
    "\n",
    "To update(approximate) the value function(network), we use\n",
    "\n",
    "$$\n",
    "MSE = (R_{t+1} + \\gamma V_v (S_{t+1}) - V_v (S_t) )^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limitations of DQN\n",
    "\n",
    "![](https://dnddnjs.gitbooks.io/rl/content/dqn16.png)\n",
    "\n",
    "The use of **replay memory** of $(s, a, r, s')$ \n",
    "\n",
    "**Advantages**\n",
    "\n",
    "+ avoidance of **non-stationary** aspect of the training data\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "+ requires a **large memory**\n",
    "+ requires **off-policy** learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A3C\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/2225DE4C58A334B62D)\n",
    "\n",
    "Asynchronous variant of actor-critic algorithm. Introduced in **_Asynchronous Methods for Deep Reinforcement Learning_** (Deepmind, 2016)\n",
    "\n",
    "To address **nonstationarity** (i.e. to introduce a stabilizing effect), it uses several sampling agents : called **actor-learner**. Each actor-learner learns in a different environment. Thus, the samples have less correlation. \n",
    "\n",
    "The global network is updated from the samples from a given time step. This process is inherently **asynchronous**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Overview\n",
    "\n",
    "1. Generate the **global network** and several **environments + actor-learners**\n",
    "2. Each actor-learner **samples** from the environment according to her model for a given time step\n",
    "3. Each actor-learner **updates** the global network with samples\n",
    "4. Each actor-learner **updates** herself from the global network\n",
    "\n",
    "### 3.2. Multithreading\n",
    "\n",
    "the ability of CPU to execute **multiple threads** concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000\n",
      "\n",
      "\n",
      "111\n",
      "\n",
      "\n",
      "22\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "class Agent(threading.Thread):\n",
    "    def _init_(self):\n",
    "        threading.Thread._init_(self)\n",
    "        pass\n",
    "    def run(self):\n",
    "        for i in range(10):\n",
    "            print (i)\n",
    "\n",
    "agents = [Agent() for i in range(3)]\n",
    "for agent in agents:\n",
    "    agent.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithreading and Generate A3C Agent class\n",
    "\n",
    "**train()** function generates classes **Agent** by the number of threads. \n",
    "\n",
    "```python\n",
    "global episode\n",
    "episode = 0\n",
    "EPISODES = 8000000\n",
    "env_name = \"BreakoutDeterministic-v4\"\n",
    "\n",
    "class A3CAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.threads = 8\n",
    "        \n",
    "    def train(self):\n",
    "        for agent in agents:\n",
    "            time.sleep(1)\n",
    "            agent.start()\n",
    "    \n",
    "    def build_model(self):\n",
    "        pass\n",
    "        \n",
    "class Agent(threading.Thread):\n",
    "    def __init__(self, action_size, state_size, model, sess,\n",
    "                 optimizer, discount_factor, summary_ops):\n",
    "        threading.Thread.__init__(self)\n",
    "        \n",
    "        # period of model updating\n",
    "        self.t_max = 20\n",
    "        self.t = 0\n",
    "    \n",
    "    def run(self):\n",
    "        global episode\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            step += 1\n",
    "            self.t += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global_agent = A3CAgent(action_size=3)\n",
    "    global_agent.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the models for the actors and the critics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ANN with the **state as an input**, **actions** (actor) or **Q function** as an output.\n",
    "\n",
    "```python\n",
    "def build_model(self):\n",
    "    input = Input(shape=self.state_size)\n",
    "    conv = Conv2D(16, (8, 8), strides=(4, 4), activation='relu')(input)\n",
    "    conv = Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    \n",
    "    # actor and critic distinguished into policy and value\n",
    "    fc = Dense(256, activation='relu')(conv)\n",
    "\n",
    "    policy = Dense(self.action_size, activation='softmax')(fc)\n",
    "    value = Dense(1, activation='linear')(fc)\n",
    "\n",
    "    actor = Model(inputs=input, outputs=policy)\n",
    "    critic = Model(inputs=input, outputs=value)\n",
    "\n",
    "    # to erase the errors entailing multithreading in Keras\n",
    "    actor._make_predict_function()\n",
    "    critic._make_predict_function()\n",
    "\n",
    "    actor.summary()\n",
    "    critic.summary()\n",
    "\n",
    "    return actor, critic\n",
    "```\n",
    "\n",
    "+ Actor \n",
    "\n",
    "Layer (type) | Output Shape | Param #\n",
    "-|-|-\n",
    "Input  | (84, 84, 4) | 0\n",
    "Conv2D | (20, 20, 16)| 4112\n",
    "Conv2D | (9, 9, 32)  | 8224\n",
    "Flatten| 2592 | 0\n",
    "Dense  | 256| 663608\n",
    "Dense  | 3  | 771\n",
    "\n",
    "+ Critic\n",
    "\n",
    "Layer (type) | Output Shape | Param #\n",
    "-|-|-\n",
    "Input  | (84, 84, 4) | 0\n",
    "Conv2D | (20, 20, 16)| 4112\n",
    "Conv2D | (9, 9, 32)  | 8224\n",
    "Flatten| 2592 | 0\n",
    "Dense  | 256| 663608\n",
    "Dense  | 1  | 257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the local network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_local_model(self):\n",
    "    input = Input(shape=self.state_size)\n",
    "    conv = Conv2D(16, (8, 8), strides=(4, 4), activation='relu')(input)\n",
    "    conv = Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    fc = Dense(256, activation='relu')(conv)\n",
    "    policy = Dense(self.action_size, activation='softmax')(fc)\n",
    "    value = Dense(1, activation='linear')(fc)\n",
    "\n",
    "    local_actor = Model(inputs=input, outputs=policy)\n",
    "    local_critic = Model(inputs=input, outputs=value)\n",
    "\n",
    "    local_actor._make_predict_function()\n",
    "    local_critic._make_predict_function()\n",
    "\n",
    "    local_actor.set_weights(self.actor.get_weights())\n",
    "    local_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "    local_actor.summary()\n",
    "    local_critic.summary()\n",
    "\n",
    "    return local_actor, local_critic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train by making threads\n",
    "\n",
    "```python\n",
    "def train(self):\n",
    "    # Generate Agent classes\n",
    "    agents = [Agent(self.action_size, self.state_size,\n",
    "                    [self.actor, self.critic], self.sess,\n",
    "                    self.optimizer, self.discount_factor,\n",
    "                    [self.summary_op, self.summary_placeholders,\n",
    "                     self.update_ops, self.summary_writer])\n",
    "              for _ in range(self.threads)]\n",
    "\n",
    "    # Start each thread\n",
    "    for agent in agents:\n",
    "        time.sleep(1)\n",
    "        agent.start()\n",
    "\n",
    "    # Save model every 10 minutes\n",
    "    while True:\n",
    "        time.sleep(60 * 10)\n",
    "        self.save_model(\"./save_model/breakout_a3c\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the actor-learner\n",
    "\n",
    "1. Choose an action according to the local network of the actor-learner\n",
    "2. Receive the next state and reward from the environment\n",
    "3. Save the samples\n",
    "4. Agent dies, or iterate by t_max timesteps\n",
    "5. Send the saved samples to the global network\n",
    "6. Global network updates itself with the samples from the local network\n",
    "7. Update the actor-learner with the updated global network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of **run()**\n",
    "\n",
    "```python\n",
    "def run(self):\n",
    "    global episode\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    while episode < EPISODES:\n",
    "        done = False\n",
    "        dead = False\n",
    "\n",
    "        score, start_life = 0, 5\n",
    "        observe = env.reset()\n",
    "        next_observe = observe\n",
    "\n",
    "        # stop for 0 - 30 states\n",
    "        for _ in range(random.randint(1, 30)):\n",
    "            observe = next_observe\n",
    "            next_observe, _, _, _ = env.step(1)\n",
    "\n",
    "        state = pre_processing(next_observe, observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        while not done:\n",
    "            step += 1\n",
    "            self.t += 1\n",
    "            observe = next_observe\n",
    "            action, policy = self.get_action(history)\n",
    "\n",
    "            # 1: stop, 2: left, 3: right\n",
    "            if action == 0:\n",
    "                real_action = 1\n",
    "            elif action == 1:\n",
    "                real_action = 2\n",
    "            else:\n",
    "                real_action = 3\n",
    "\n",
    "            # if dead, shoot to restart\n",
    "            if dead:\n",
    "                action = 0\n",
    "                real_action = 1\n",
    "                dead = False\n",
    "\n",
    "            # take one step with chosen action\n",
    "            next_observe, reward, done, info = env.step(real_action)\n",
    "\n",
    "            # pre-process the state at each timestep\n",
    "            next_state = pre_processing(next_observe, observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3],\n",
    "                                     axis=3)\n",
    "\n",
    "            # take max of the policy\n",
    "            self.avg_p_max += np.amax(self.actor.predict(\n",
    "                np.float32(history / 255.)))\n",
    "\n",
    "            if start_life > info['ale.lives']:\n",
    "                dead = True\n",
    "                start_life = info['ale.lives']\n",
    "\n",
    "            score += reward\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "            # save the sample\n",
    "            self.append_sample(history, action, reward)\n",
    "\n",
    "            if dead:\n",
    "                history = np.stack((next_state, next_state,\n",
    "                                    next_state, next_state), axis=2)\n",
    "                history = np.reshape([history], (1, 84, 84, 4))\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            # start learning, if dead or t_max reached\n",
    "            if self.t >= self.t_max or done:\n",
    "                self.train_model(done)\n",
    "                self.update_local_model()\n",
    "                self.t = 0\n",
    "\n",
    "            if done:\n",
    "                # record the learned info at each time step\n",
    "                episode += 1\n",
    "                print(\"episode:\", episode, \"  score:\", score, \"  step:\",\n",
    "                      step)\n",
    "\n",
    "                stats = [score, self.avg_p_max / float(step),\n",
    "                         step]\n",
    "                for i in range(len(stats)):\n",
    "                    self.sess.run(self.update_ops[i], feed_dict={\n",
    "                        self.summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "                summary_str = self.sess.run(self.summary_op)\n",
    "                self.summary_writer.add_summary(summary_str, episode + 1)\n",
    "                self.avg_p_max = 0\n",
    "                self.avg_loss = 0\n",
    "                step = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the optimizer to update the actor\n",
    "\n",
    "```python\n",
    "def actor_optimizer(self):\n",
    "    action = K.placeholder(shape=[None, self.action_size])\n",
    "    advantages = K.placeholder(shape=[None, ])\n",
    "\n",
    "    policy = self.actor.output\n",
    "\n",
    "    # Policy cross-entropy loss function\n",
    "    action_prob = K.sum(action * policy, axis=1)\n",
    "    cross_entropy = K.log(action_prob + 1e-10) * advantages\n",
    "    cross_entropy = -K.sum(cross_entropy)\n",
    "\n",
    "    # Entropy error to facilitate exploration\n",
    "    entropy = K.sum(policy * K.log(policy + 1e-10), axis=1)\n",
    "    entropy = K.sum(entropy)\n",
    "\n",
    "    # Final loss = sum of two losses\n",
    "    loss = cross_entropy + 0.01 * entropy\n",
    "\n",
    "    optimizer = RMSprop(lr=self.actor_lr, rho=0.99, epsilon=0.01)\n",
    "    updates = optimizer.get_updates(self.actor.trainable_weights, [],loss)\n",
    "    train = K.function([self.actor.input, action, advantages],\n",
    "                       [loss], updates=updates)\n",
    "    return train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy** as a loss. To minimize the entropy, actor-learner should **equalize the policy**.\n",
    "\n",
    "$$\n",
    "Entropy = - \\sum_i p_i \\log p_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the k-time step advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage function used in actor-critic is **one-step temporal difference error**.\n",
    "\n",
    "$$\n",
    "Advantage = R_{t+1} + \\gamma V_v (S_{t+1}) - V_v (S_t)\n",
    "$$\n",
    "\n",
    "**k-time step advantage function** calculates the advantage function after several time steps. This calibration of the advantage after k-timesteps is called **multi-step TD learning**, which is midway between SARSA and Monte Carlo.\n",
    "\n",
    "$$\n",
    "Advantage = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^k V_v (S_{t+k}) - V_v (S_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def discounted_prediction(self, rewards, done):\n",
    "        discounted_prediction = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "\n",
    "        if not done:\n",
    "            running_add = self.critic.predict(np.float32(\n",
    "                self.states[-1] / 255.))[0]\n",
    "\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_prediction[t] = running_add\n",
    "        return discounted_prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the optimizer to update the critic\n",
    "\n",
    "```python\n",
    "def critic_optimizer(self):\n",
    "    discounted_prediction = K.placeholder(shape=(None,))\n",
    "\n",
    "    value = self.critic.output\n",
    "\n",
    "    # set loss as the square of [return - value]\n",
    "    loss = K.mean(K.square(discounted_prediction - value))\n",
    "\n",
    "    optimizer = RMSprop(lr=self.critic_lr, rho=0.99, epsilon=0.01)\n",
    "    updates = optimizer.get_updates(self.critic.trainable_weights, [],loss)\n",
    "    train = K.function([self.critic.input, discounted_prediction],\n",
    "                       [loss], updates=updates)\n",
    "    return train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Loss = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^k V_v (S_{t+k}) - V_v (S_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the global network\n",
    "\n",
    "```python\n",
    "def train_model(self, done):\n",
    "    discounted_prediction = self.discounted_prediction(self.rewards, done)\n",
    "\n",
    "    states = np.zeros((len(self.states), 84, 84, 4))\n",
    "    for i in range(len(self.states)):\n",
    "        states[i] = self.states[i]\n",
    "\n",
    "    states = np.float32(states / 255.)\n",
    "\n",
    "    values = self.critic.predict(states)\n",
    "    values = np.reshape(values, len(values))\n",
    "\n",
    "    advantages = discounted_prediction - values\n",
    "\n",
    "    self.optimizer[0]([states, self.actions, advantages])\n",
    "    self.optimizer[1]([states, discounted_prediction])\n",
    "    self.states, self.actions, self.rewards = [], [], []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the local network\n",
    "\n",
    "```python\n",
    "def update_local_model(self):\n",
    "        self.local_actor.set_weights(self.actor.get_weights())\n",
    "        self.local_critic.set_weights(self.critic.get_weights())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
