{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network : actualizations\n",
    "\n",
    "## 1. DQN : basics\n",
    "\n",
    "### Experience replay\n",
    "\n",
    "**Replay memory** contains samples of $(s, a, r, s')$\n",
    "\n",
    "**Advantages** \n",
    "\n",
    "+ **More efficient use of previous experience**, by learning with it multiple times, especially when gaining real-world experience is costly. The Q-learning updates are incremental and do not converge quickly, so multiple passes with the same data is beneficial, especially when there is low variance in immediate outcomes (reward, next state) given the same state, action pair.\n",
    "\n",
    "+ **Better convergence behavior**(**stability**) when training a function approximator. Partly this is because the data is more like i.i.d. data assumed in most supervised learning convergence proofs. \n",
    "\n",
    "**Disadvantage**\n",
    "\n",
    "+ It is harder to use **multi-step learning algorithms**, such as $Q(\\lambda)$, which can be tuned to give better learning curves by balancing between bias (due to bootstrapping) and variance (due to delays and randomness in long-term outcomes). Multi-step DQN with experience-replay DQN is one of the extensions explored in the paper **Rainbow**: Combining Improvements in Deep Reinforcement Learning.\n",
    "\n",
    "\n",
    "### Target network\n",
    "\n",
    "**DQN loss function**\n",
    "\n",
    "$$\n",
    "MSE = \\big( R_{t+1} + \\gamma \\max_{a'} Q(s', a', \\theta) - Q(s, a, \\theta) \\big)^2\n",
    "$$\n",
    "\n",
    "using the **Bellman error** at state $s$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{\\delta}_w (s) & \\overset{def}{=} \\Big( \\sum_a \\pi (a | s) \\sum_{s', r'} p (s', r | s, a) [r + \\gamma v_w (s')]  \\Big)  - v_w (s) \\\\[10pt]\n",
    "&= \\mathbb{E} \\big[ R_{t+!} + \\gamma v_w (S_{t+1}) - v_w (S_t) | S_t = s, A_t \\sim \\pi \\big] \\\\[15pt]\n",
    "\\bar{BE}(w) & \\overset{def}{=} ||\\bar{\\delta}_w (s) ||^2_\\mu\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**DQN loss function** using the **target network**\n",
    "\n",
    "$$\n",
    "MSE = \\big( R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a', \\theta^-) - Q(S_t, A_t, \\theta) \\big)^2\n",
    "$$\n",
    "\n",
    "+ **Neural network** has the parameters $\\theta$\n",
    "+ The network for the **ground truth** (**target network**) has the parameters $\\theta^-$ and stays for a given time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem formulation\n",
    "\n",
    "**Goal** : **keep the pole vertical**, in order for the pole not to sland **AND** for the cart not to escape the screen.\n",
    "\n",
    "**State** : $\n",
    "\\Biggl[ \\begin{matrix}\n",
    "\\mathbf{x} \\\\ \\dot{\\mathbf{x}} \\\\ \\theta \\\\ \\dot{\\theta} \\end{matrix} \\Biggl]$, where $\\mathbf{x}$ : position, $\\dot{\\mathbf{x}}$ : speed, $\\theta$ : angle, $\\dot{\\theta}$ : angular speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm\n",
    "\n",
    "### Overview\n",
    "\n",
    "1. Choose an **action** from observing the **state**\n",
    "\n",
    "2. **Take a time step** in the environment with the chosen action\n",
    "\n",
    "3. Get the **next state** and the **reward** from the environment\n",
    "\n",
    "4. **Save the sample** $(s, a, r, s')$ in the **replay memory**\n",
    "\n",
    "5. **Learn** with the **randomly chosen samples** from the replay memory\n",
    "\n",
    "6. **Update the target network** at every episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an agent & load the env\n",
    "\n",
    "```python\n",
    "    import gym\n",
    "    from gym import wrappers\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        env = wrappers.Monitor(env,\"./cartpole-experiment/\",force=True, video_callable=lambda episode_id: e%10==0)\n",
    "\n",
    "        state_size = env.observation_space.shape[0]  ## 4\n",
    "        action_size = env.action_space.n             ## 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a neural network\n",
    "\n",
    "Create an ANN with the **state as an input**, **Q-fun as an output**\n",
    "\n",
    "```python\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "```\n",
    "\n",
    "Layer (type) | Output Shape | Param #\n",
    "-|-|-\n",
    "Dense1 | 24 | 120\n",
    "Dense2 | 24 | 600\n",
    "Dense3 | 2  | 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a step\n",
    "\n",
    "**get_action**(state) : $\\varepsilon$-greedy\n",
    "\n",
    "```python\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "            \n",
    "            \n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save samples in the replay memory\n",
    "\n",
    "```python\n",
    "    self.memory = deque(maxlen=2000)\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "```python\n",
    "    if len(agent.memory) >= agent.train_start:\n",
    "        agent.train_model()\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            dones.append(mini_batch[i][4])\n",
    "            \n",
    "        # Q-fun of the model for the current state\n",
    "        # Q-fun of the target net for the next state\n",
    "        target = self.model.predict(states)\n",
    "        target_val = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        self.model.fit(states, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
