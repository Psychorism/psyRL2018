{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP & RL and Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DP : Value Iteration\n",
    "\n",
    "## 1.1. Explicit vs. Implicit Policies\n",
    "\n",
    "It assumes a **deterministic policy**. Thus, if we update the value function, the **policy automatically improves**.\n",
    "\n",
    "## 1.2. Bellman Optimality Equation and Value Iteration & Code\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "1. Calcualte the **next value function** using **Bellman Optimality Equation**\n",
    "\n",
    "2. **Return the action** from the present value function.\n",
    "\n",
    "**Backgrounds & Proofs**\n",
    "\n",
    "If **policy evaluation** is done iteratively, then **convergence exactly to $v_{\\pi}$ in the limit**.\n",
    "\n",
    "In fact, **policy evaluation step of policy iteration can be truncated** without losing the convergence guarantees.\n",
    "\n",
    "One important special case is when **policy evaluation is stopped after just one sweep**. (GPI)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{k+1} (s) &\\overset{def}{=} \\underset{a}{max} \\mathbb{E} \\Big[ R_{t+1} + \\gamma v_k (S_{t+1}) \\mid S_t =s, A_t =a \\Big] \\\\[10pt]\n",
    "&= \\underset{a}{max} \\sum_{s', r} p (s', r \\mid s, a) \\Big[ r + \\gamma v_k (s') \\Big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The **sequence $\\{v_k\\}$ converges to $v_*$**, under the same conditions that guarantees the existence of $v_*$\n",
    "\n",
    "(**Proof**)\n",
    "\n",
    "Define the following **Bellman backup operator** $B (\\hat{V}) : \\mathbb{R}^{|S|} \\rightarrow \\mathbb{R}^{|S|}$, for any estimate value function $\\hat{V}$.\n",
    "\n",
    "$$\n",
    "B \\big[ \\hat{V}(s) \\big] = R(s) + \\gamma \\underset{a \\in A}{max} \\sum_{s'} p (s' \\mid s, a) \\hat{V}(s')\n",
    "$$\n",
    "\n",
    "**Claim** : Bellman backup operator is a **contraction**. That is, $\\forall V_1, V_2$\n",
    "\n",
    "$$\n",
    "\\underset{s \\in S}{max} \\ \\ \\Big| B \\big[ V_1(s) \\big] - B \\big[ V_2(s) \\big] \\Big| \\le \\underset{s \\in S}{max} \\ \\ \\Big| V_1(s) - V_2(s) \\Big|\n",
    "$$\n",
    "\n",
    "(Proof of **contraction property**)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Big| B \\big[ V_1(s) \\big] - B \\big[ V_2(s) \\big] \\Big| &= \\gamma \\big| \\ \\  \\underset{a \\in A}{max} \\sum_{s'} p (s' \\mid s, a) V_1(s') - \\underset{a \\in A}{max} \\sum_{s'} p (s' \\mid s, a) V_2(s') \\ \\ \\big| \\\\[10pt]\n",
    "&\\le \\gamma \\big| \\ \\  \\underset{a \\in A}{max} \\sum_{s'} p (s' \\mid s, a) V_1(s') - \\underset{a \\in A}{max} \\sum_{s'} p (s' \\mid s, a) V_2(s') \\ \\ \\big| \\\\[10pt]\n",
    "&= \\gamma \\underset{a \\in A}{max} \\sum_{s'} p (s' \\mid s, a) \\big| V_1(s') - V_2(s') \\big| \\\\[10pt]\n",
    "&= \\gamma \\underset{s \\in S}{max} \\big| V_1(s) - V_2(s) \\big|\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Result** \n",
    "\n",
    "$$\n",
    "\\underset{s \\in S}{max} \\ \\ \\Big| B \\big[ \\hat{V}(s) \\big] - B \\big[ V_*(s) \\big] \\Big| \\le \\underset{s \\in S}{max} \\ \\ \\Big| \\hat{V}(s) - V_*(s) \\Big| \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\hat{V}(s) \\rightarrow V_*(s)\n",
    "$$\n",
    "\n",
    "**Pitfalls of DP**\n",
    "\n",
    "1. Computational **Complexity**\n",
    "\n",
    "2. Curse of **Dimentionality**\n",
    "\n",
    "3. Requirement of the **Complete Information** (i.e. **Model**) of the environment, where **Model** means $\\mathcal{P}^a_{ss'}, \\mathcal{R}^a_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Monte Carlo Prediction\n",
    "\n",
    "## 2.1. Prediction and Control in RL\n",
    "\n",
    "(Because all the action selections are undergoing learning, the problem becomes **nonstationary from the point of view of the earlier state**.)\n",
    "\n",
    "|Dynamic Programming|Monte Carlo\n",
    "-|-|-\n",
    "**Finding the True Value**|Evaluation|Prediction\n",
    "**Improving the Policy**|Improvement|Control\n",
    "\n",
    "## 2.2. Monte Carlo Approximation\n",
    "\n",
    "Justified by the **Law of Large Numbers(LLN)**\n",
    "\n",
    "## 2.3. Sampling and Monte Carlo Prediction\n",
    "\n",
    "**Goal** : To find the **expectation of return** ($G_t$)\n",
    "\n",
    "we can find the expectation by **DP**(recurrent formula), or **Monte Carlo**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi} (s) = E_{\\pi} \\Big[ R_{t+1} + \\gamma R_{t+1} + \\cdots \\mid S_t = s\\Big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### How to calculate the expected return?\n",
    "\n",
    "Method|Calculation of Return\n",
    "-|-\n",
    "**First-visit Monte-Carlo Policy evaluation** | calculate return from the **first visit in an episode**\n",
    "**Every-visit Monte-Carlo Policy evaluation** | calculate return from all the **visits in an episode**\n",
    "\n",
    "Both converge to the **true value** as $N(s) \\rightarrow \\infty$\n",
    "\n",
    "**Our approach**?\n",
    "\n",
    "### Incremental Mean\n",
    "\n",
    "The **means** of the **sequence** $x_1, x_2, \\cdots$ \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_k &= \\frac{1}{k} \\sum_{i=1}^k x_i \\\\\n",
    "&= \\frac{1}{k} \\Big( x_k + \\sum_{i=1}^{k-1} x_i \\Big)\\\\\n",
    "&= \\frac{1}{k} \\Big( x_k + (k-1) \\mu_{k-1} \\Big)\\\\\n",
    "&= \\mu_{k-1} + \\frac{1}{k} ( x_k - \\mu_{k-1} )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similarly, update $V(s_t)$ for each $S_t$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "N(S_t) &\\leftarrow N(S_t) + 1 \\\\\n",
    "V(S_t) &\\leftarrow V(S_t) + \\frac {1}{N(S_t)} (G_t - V(S_t) )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "**General Formula**\n",
    "\n",
    "$$\n",
    "V(s) \\leftarrow V(s) + \\alpha (G(s) - V(s) )\n",
    "$$\n",
    "\n",
    "1. The **Goal** of updates : $G(s)$\n",
    "\n",
    "2. The **Size** of updates : $\\alpha ( G(s) - V(s) )$\n",
    "\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "1. After an episode, update the **q function** for **every visited state**.\n",
    "\n",
    "2. Return the next action, following the **q function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Temporal Difference Prediction\n",
    "\n",
    "## 3.1. TD Prediction\n",
    "\n",
    "**Pitfalls of MC** : **Not real-time**, that is, **an update per episode**\n",
    "\n",
    "TD updates : done in real time, **one value function at a time**.\n",
    "\n",
    "Converges to **true value function** faster than MC.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) = E_{\\pi} [R_{t+1} V(S_t) + \\alpha (R + \\gamma V(S_{t+1}) - V(S_t) )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\Big(R + \\gamma V(S_{t+1}) - V(S_t) \\Big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "1. The **Goal** of updates : $R + \\gamma V(S_{t+1})$\n",
    "\n",
    "2. The **Size** of updates : $\\alpha \\Big(R + \\gamma V(S_{t+1} ) - V(S_t) \\Big)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
