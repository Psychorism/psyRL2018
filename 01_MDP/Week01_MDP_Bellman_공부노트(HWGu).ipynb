{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process & Bellman Equation\n",
    "\n",
    "## 1. Markov Decision Process(MDP)\n",
    "\n",
    "MDP is a **discrete time stochastic control process**. It is a mathematical definition of a sequential decision making process.\n",
    "    \n",
    "+ stochastic process : a collection of **random variables**\n",
    "\n",
    "By definition, a MDP is a 5-tuple $(\\mathcal{S},\\mathcal{A},\\mathcal{P}_a,\\mathcal{R}_a,\\gamma)$.\n",
    "\n",
    "1. **State** ($S \\in \\mathcal{S}$)\n",
    "\n",
    "2. **Action** ($A \\in \\mathcal{A}$)\n",
    "\n",
    "    - $A_s$ is the finite set of actions available from state\n",
    "\n",
    "3. **(State) Transition Probability** ($\\mathcal{P}_a$)\n",
    "\n",
    "    - $\\mathcal{P}^a_{ss'} = Pr[S_{t+1} = s' \\mid S_t = s, A_t = a]$ \n",
    "    \n",
    "    - the probability that action $a$ in state $s$ at time $t$ will lead to state $s'$ at time $(t+1)$.\n",
    "\n",
    "4. **Reward Function** ($\\mathcal{R}^a_s$)\n",
    "\n",
    "    - $\\mathcal{R}^a_s = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]$\n",
    "\n",
    "    - the expected immediate reward received after transitioning from state $s$ to state $s'$, due to action $a$\n",
    "    \n",
    "5. **Discount Factor** ($\\gamma$)\n",
    "\n",
    "    - it represents the difference in importance between future rewards and present rewards.\n",
    "\n",
    "    - used to avoid the problem of infinite return ($G_t = R_{t+1} + \\gamma R_{t+1} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$)\n",
    "    \n",
    "**Policy** ($\\pi$)\n",
    "\n",
    "- The core problem of MDPs is to find a **policy** for the decision maker.\n",
    "\n",
    "- $\\pi(a \\mid s) = Pr [A_t = a \\mid S_t =s ]$\n",
    "\n",
    "- the mapping from states to probabilities of selecting each possible action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Value Function\n",
    "\n",
    "Almost all RL algorithms involve estimating **value functions**.\n",
    "\n",
    "**Value function** is the function of states that estimated how good it is for the agent to be in a given state.\n",
    "\n",
    "The notion of \"how good\" is defined in terms of **expected return**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[G_t \\mid S_t =s ] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t =s] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t =s ] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t =s ] \\\\[15pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let us define the value of taking action $a$ in state $s$ under a policy $\\pi$, denoted $q_{\\pi}(s,a)$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s,a) &= \\mathbb{E}_{\\pi}[G_t \\mid S_t =s, A_t = a] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t =s, A_t = a] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t =s, A_t = a] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) \\mid S_t =s, A_t = a]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bellman Equation\n",
    "\n",
    "### Bellman Expectation Equation\n",
    "\n",
    "It elucidates the **relationship** between the present state value function and the next state value function.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s)  &= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t =s ] \\\\[10pt]\n",
    "&= \\sum_{a \\in \\mathcal{A}} \\pi (a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}_a} Pr(s', r \\mid s, a) \\bigl[ r + \\gamma \\mathbb{E}_{\\pi} [G_{t+1} \\mid S_{t+1} = s'] \\bigr] \\\\[10pt]\n",
    "&= \\sum_{a \\in \\mathcal{A}} \\pi (a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}_a} Pr(s', r \\mid s, a) \\bigl[ r + \\gamma v_{\\pi}(s') \\bigr] \\\\[10pt]\n",
    "& \\text{(if we assume the identical structure of reward function)} \\\\[10pt]\n",
    "&= \\sum_{a \\in \\mathcal{A}} \\pi (a \\mid s) \\bigl(R_{t+1} + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'} v_{\\pi}(s')\\bigr) \\\\[10pt]\n",
    "& \\text{(if we further assume the identical structure of transition probability)} \\\\[10pt]\n",
    "&= \\sum_{a \\in \\mathcal{A}} \\pi (a \\mid s) \\bigl(R_{t+1} + \\gamma \\sum_{s' \\in \\mathcal{S}} v_{\\pi}(s')\\bigr)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Bellman Optimality Equation\n",
    "\n",
    "Bellman expectation equation only provides **the true value function under the present policy**. \n",
    "\n",
    "When the goal is to find **\"optimal policy\"**, we would like to find the **optimal value function**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^{*}(s) &= \\max_{a \\in \\mathcal{A}_s} q_{\\pi^{*}} (s, a) \\\\[10pt]\n",
    "&= \\max_{a \\in \\mathcal{A}_s} \\mathbb{E}_{\\pi^{*}} [R_{t+1} + \\gamma v^{*}(S_{t+1}) \\mid S_t = s, A_t = a] \\cdots \\text{ (#)} \\\\[10pt]\n",
    "&\\text{(#) } q^{*} (s, a) = \\mathbb{E} [R_{t+1} + \\gamma v^{*} (S_{t+1}) \\mid S_t = s, A_t = a] \\\\[10pt]\n",
    "&= \\max_{a \\in \\mathcal{A}_s} \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}_a} Pr(s', r \\mid s, a) \\bigl[ r + \\gamma v^{*}(s') \\bigr] \\\\[10pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Bellman optimality equation for Q functions** is as follows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q^{*}(s,a) &= \\mathbb{E} [R_{t+1} + \\gamma \\max_{a'} q^{*} (S_{t+1}, a') \\mid S_t = s, A_t = a] \\\\[10pt]\n",
    "&= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}_a} Pr(s', r \\mid s, a) \\bigl[ r + \\gamma \\max_{a'} q^{*} (s', a') \\bigr] \\\\[10pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Dynamic programming** (DP) solves the problems in the form of MDP **by calculating** with Bellman expectation & optimality equations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
