{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "## 1. Dynamic Programming & Grid World\n",
    "\n",
    "### 1.1. Recap : MDP\n",
    "\n",
    "MDP is a **discrete time stochastic process**. \n",
    "\n",
    "An **MDP** is a 5-tuple $(\\mathcal{S},\\mathcal{A},\\mathcal{P}_a,\\mathcal{R}_a,\\gamma)$.\n",
    "\n",
    "**That is**, $(S_1,A_1,\\mathcal{P}^{a_1}_{s_1 s'},\\mathcal{R}^{a_1}_{s_1},\\gamma)$, $(S_2,A_2,\\mathcal{P}^{a_2}_{s_2 s'},\\mathcal{R}^{a_2}_{s_2},\\gamma)$, $(S_3,A_3,\\mathcal{P}^{a_3}_{s_3 s'},\\mathcal{R}^{a_3}_{s_3},\\gamma)$, $\\cdots$ until the **stopping time $\\tau$**.\n",
    "\n",
    "1. **State** ($S \\in \\mathcal{S}$)\n",
    "\n",
    "2. **Action** ($A \\in \\mathcal{A}$)\n",
    "\n",
    "    - $A_s$ is the finite set of actions available from state\n",
    "\n",
    "3. **(State) Transition Probability** ($\\mathcal{P}_a$)\n",
    "\n",
    "    - $\\mathcal{P}^a_{ss'} = Pr[S_{t+1} = s' \\mid S_t = s, A_t = a]$ \n",
    "    \n",
    "    - the probability that action $a$ in state $s$ at time $t$ will lead to state $s'$ at time $(t+1)$.\n",
    "\n",
    "4. **Reward Function** ($\\mathcal{R}^a_s$)\n",
    "\n",
    "    - $\\mathcal{R}^a_s = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]$\n",
    "\n",
    "    - the expected immediate reward received after transitioning from state $s$ to state $s'$, due to action $a$\n",
    "    \n",
    "5. **Discount Factor** ($\\gamma$)\n",
    "\n",
    "    - it represents the difference in importance between future rewards and present rewards.\n",
    "\n",
    "    - used to avoid the problem of infinite return ($G_t = R_{t+1} + \\gamma R_{t+1} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$)\n",
    "    \n",
    "### 1.2. A Sequential Decision Making Problem : MDP vs. Reality\n",
    "\n",
    "**Example of \"Street Dancing\"**\n",
    "\n",
    "1. **State**($S_t$) : a specific **posture** at time point $t$\n",
    "\n",
    "2. **Action**($A_t$) : a specific **movement** at time point $t$\n",
    "\n",
    "3. **Transition Probability**($P^{a_t}_{s_t s'}$) : the probability that **movement** $a_t$ from the **posture** $s_t$ will lead to the next **posture** $s'$.\n",
    "\n",
    "4. **Reward Function**($R^{a_t}_{s_t}$) : the amount of **applause** from the crowd received from from the **posture** $s_t$ to $s'$, due to the **movement** $a_t$.\n",
    "\n",
    "5. **Discount Factor** ($\\gamma$) : the difference in importance between future **applause** and present **applause**.\n",
    "\n",
    "(Note that **1,2,4** are the **random variables**)\n",
    "\n",
    "|MDP|Reality\n",
    "-|-|-\n",
    "$\\mathcal{S}$|known|known\n",
    "$\\mathcal{A}$|known|known\n",
    "$\\mathcal{P}$|**known beforehand**|**unknown**\n",
    "$\\mathcal{R}$|**known beforehand**|**unknown until observed**\n",
    "$\\gamma$|known|known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Dynamic Programming (DP)\n",
    "\n",
    "**What is DP**?\n",
    "\n",
    "+ A problem-solving paradigm, where the problem is **split into several sub-problems** and the **past solutions are reused**.\n",
    "\n",
    "+ If sub-problems can be **nested recursively** inside larger problems, so that dynamic programming methods are applicable, then **there is a relation between the value of the larger problem and the values of the sub-problems**.\n",
    "\n",
    "**Why DP**?\n",
    "\n",
    "+ In order to effectively find the **target function**.\n",
    "\n",
    "+ To solve the **recurrence formula** by dividing the problem into smaller processes\n",
    "\n",
    "$$\n",
    "v_0(s) \\rightarrow v_1(s) \\rightarrow \\cdots \\rightarrow v_k(s) \\rightarrow \\cdots \\rightarrow v_{\\pi}(s)\n",
    "$$\n",
    "\n",
    "**How to find the value function?**\n",
    "\n",
    "+ **Value function** is the function of states that estimated how good it is for the agent to be in a given state.\n",
    "\n",
    "+ The notion of \"how good\" is defined in terms of **expected return**.\n",
    "\n",
    "+ Thus, we can adopt a **quick-and-dirty approach** using simulations.\n",
    "\n",
    "### 1.4. Grid World Example (Quick-and-Dirty)\n",
    "\n",
    "Let us assume a $3 \\times 3$ grid world.\n",
    "\n",
    "For simplicity, we can assume the following.\n",
    "\n",
    "$$\\mathcal{P}_a = 1,\\mathcal{R}_a = 0, \\text{ (in terminal, } \\mathcal{R}_a = 1) ,\\gamma=0.7$$\n",
    "\n",
    "And we have **random policy**, which can be defined as\n",
    "\n",
    "$$\n",
    "\\pi(a \\mid s) = 0.25 \\ \\ \\forall a\n",
    "$$\n",
    "\n",
    "Without DP, we still be able to **approximate $v$ function**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0 \tS=(0, 0) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=1 \tS=(1, 0) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=2 \tS=(2, 0) \tA=up \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=3 \tS=(1, 0) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=4 \tS=(2, 0) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=5 \tS=(2, 0) \tA=up \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=6 \tS=(1, 0) \tA=east \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=7 \tS=(1, 1) \tA=up \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=8 \tS=(0, 1) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=9 \tS=(1, 1) \tA=east \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=10 \tS=(1, 2) \tA=down \tP=1 \tR=0.0 \tgamma=0.7\n",
      "t=11 \tS=(2, 2) \t \t \tR=1.0 \tgamma=0.7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f900c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<DP.randomPolicy at 0x10f9008d0>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from DP import *\n",
    "\n",
    "env = Env()\n",
    "randomPolicy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03049462  0.06086549  0.11517023]\n",
      " [ 0.05608203  0.13407214  0.28155053]\n",
      " [ 0.10685116  0.26798336  0.7       ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DP.Simulator at 0x11657cf28>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Simulator(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamic Programming 1 : Policy Iteration\n",
    "\n",
    "### 2.1. Flow of RL algorithms\n",
    "\n",
    "**Equation**|**Algorithm**|\n",
    "-|-\n",
    "**Bellman Expectation Equation** | Policy iteration\n",
    "**Bellman Optimality Equation** | Value iteration\n",
    "\n",
    "\n",
    "### 2.2. Policy Iteration\n",
    "\n",
    "+ **Policy** $\\rightarrow$ **Policy Evaluation** $\\rightarrow$ **Policy Improvement**\n",
    "\n",
    "+ This iteration can make the policy to converge to the **optimal policy**.\n",
    "\n",
    "### 2.3. Policy Evaluation\n",
    "\n",
    "We can redefine **Bellman Expectation Equation** in a **recursive form**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[G_t \\mid S_t =s ] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t =s] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t =s ] \\\\[10pt]\n",
    "&= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t =s ] \\\\[15pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This can be further modified so that it can be **calculated**. Note **how $v$ is dependent upon $\\pi$**.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t =s ] \\\\[15pt]\n",
    "&= \\sum_{a \\in A} \\pi (a \\mid s) (R_{t+1} + \\gamma v_{\\pi}(s') ) ) \\\\[15pt]\n",
    "v_{k+1}(s) &= \\sum_{a \\in A} \\pi (a \\mid s) (R^{a}_{s} + \\gamma v_{k}(s') ) ) \\\\[15pt]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 2.4. Grid World Example (DP)\n",
    "\n",
    "Using the above formula, we can **easily calculate the $v$ function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.09515625  0.29375   ]\n",
      " [ 0.04375     0.25        0.666129  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DP.Bellman at 0x10a4819e8>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bellman(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Policy Improvement\n",
    "\n",
    "How can we improve our policy based on the **policy evaluation**?\n",
    "\n",
    "We introduce **Greedy Policy Improvement**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi} (s,a) &= R^{a}_{s} + \\gamma v_{\\pi} (s') \\\\[15pt]\n",
    "\\pi'(s) &= argmax_{a \\in A} q_{\\pi} (s, a)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
