{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL in Psychology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prediction and Control\n",
    "\n",
    "+ Pavlovian conditioning \n",
    "    + Prediction algorithms : policy evaluation algorithms, or prediction of any feature of the environment\n",
    "\n",
    "+ Operant conditioning\n",
    "    + Reinforcing stimulus is contingent on the animal’s behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pavlovian Conditioning \n",
    "\n",
    "### 2.1. Blocking and Higher-order conditioning\n",
    "\n",
    "Higher-order conditioning occurs when a **previously-conditioned CS** acts as a **US** in conditioning another initially neutral stimulus. Pavlov described an experiment in which his assistant first conditioned a dog to salivate to the sound of a metronome that predicted a food US, as described above.\n",
    "\n",
    "As we describe below, the TD model of classical conditioning uses the **bootstrapping idea**, which extends to the Rescorla–Wagner model’s account of blocking to include both the anticipatory nature of CRs and higher-order conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The Rescorla-Wagner Model\n",
    "\n",
    "An animal only learns **when events violate its expectations**, i.e. only when the animal is surprise.\n",
    "\n",
    "Consider a **compound CS** $AX$, where $A$ : experienced and $X$ : new. Let $V_A$, $V_X$, $V_{AX}$ denote the associative strengths of stimuli $A, X, AX$. Suppose on a trial the compund CS $AX$ is followed by a US $Y$. Then the associative strengths of the stimulus components change according to these expressions :\n",
    "\n",
    "$$\n",
    "\\Delta V_A = \\alpha_A \\beta_Y (R_Y - V_{AX}) \\\\\n",
    "\\Delta V_X = \\alpha_X \\beta_Y (R_Y - V_{AX}) \\\\\n",
    "$$\n",
    "\n",
    "where $\\alpha_A \\beta_Y, \\alpha_X \\beta_Y$ are step-size parameters, which depend on the identities of CS components and US, $R_Y$ : asymptotic level of associative strength the US $Y$ can support. **Note that** the aggregate associative strength $V_{AX} = V_A + V_X$\n",
    "\n",
    "**RW Model** accounts for the acquisition of CRs in a way that explains blocking. **US prediction**, which corresponds to a _value estimate_ in RL is as follows.\n",
    "\n",
    "$$\n",
    "\\hat{v} (s, \\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s)\n",
    "$$\n",
    "\n",
    "where $t$ : time step, and state $s$ is described by a **feature vector** $\\mathbf{x} = (x_1(s), \\cdots, x_d(s))^T$ with $x_i(s) = 1$ only if ith component of a compound CS is present on the trial. So the above formula means the **aggregate associative strength** for trial-type $s$.\n",
    "\n",
    "Conditioning trial $t$ updates the associative strength vector $\\mathbf{w}_t$ to $\\mathbf{w}_{t+1}$ as follows.\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} := \\mathbf{w}_t + \\alpha \\delta_t \\mathbf{x} (S_t)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ : step-size parameter, $\\delta_t$ : prediction error. \n",
    "\n",
    "$$\n",
    "\\delta_t = R_t - \\hat{v} (S_t, \\mathbf{w}_t)\n",
    "$$\n",
    "\n",
    "where $R_t$ is the target of the prediction on trial $t$, i.e. the magnitude of the US (associative strength that US on the trial can support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. The TD Model\n",
    "\n",
    "A real-time model, which includes **higher-order conditioning** as a natural consequence of the **bootstrapping** idea (i.e. **updating** the state-value estimates **based on estimates** of the values of successor states). The following formula replaces \n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\delta_t \\mathbf{z}_t\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}$ : associative strength vector, $t$ : time-step\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v} (S_t, \\mathbf{w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cognitive Maps\n",
    "\n",
    "+ **Environment model** : anything an agent can use to predict how its environment will respond to its actions in terms of state transitions and rewards\n",
    "\n",
    "+ **Planning** : any process that computes a policy from such a model\n",
    "\n",
    "Tolman : **expectancy theory** L given S-S associations, the occurrence of a stimulus generates an expectation about the stimulus to come next. (**system identification**). These are all forms of SL by which an agent can acquire cognitive-like maps whether or not it receives any non-zero reward signals while exploring its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Habitual and Goal-directed Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two strategies to solve a hypothetical **sequential action-selection problem**\n",
    "\n",
    "+ **Model-free** decision strategy : relies on **stored action values** for all the **state-action pairs** obtained over many learning trials.\n",
    "\n",
    "+ **Model-based** decision strategy : the rat learns an environment model, consisting of knowledge of $S-A-S'$ transitions and a reward model consisting of knowledge of the reward associated with each distinctive goal box\n",
    "\n",
    "#### Extra) Stress enhances model-free RL only after negative outcome (H. Park, D. Lee, J. Choi, 2017)\n",
    "\n",
    "+ **Two-stage Markov decision task** (Daw _et al._, 2011) to distinguish the contribution of model-free and model-based RL to action seleciton.\n",
    "\n",
    "+ **Reversal learning paradigm** for participants to face a changing environment \n",
    "\n",
    "In the Q-learning model, action values are updated via a simple **Rescorla-Wagner(RW) rule**.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_{t+1} (x) &= V_t (x) + \\alpha (R_t - V_t(x) ) \\\\[7pt]\n",
    "&= (1-\\alpha) V_t(x) + \\alpha R_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this model, the action values are updated according to the following\n",
    "\n",
    "$$\n",
    "V_{t+1} (x) = \\begin{cases} (1-\\alpha) V_t (x) + \\alpha \\kappa_+ & \\text{if rewarded} \\\\[7pt]\n",
    "(1-\\alpha) V_t (x) + \\alpha \\kappa_- & \\text{if not rewarded} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of choosing each option is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P_t(a_1) = 1/\\Big( 1 + exp \\big( - (V_t (a_1) - V_t (a_2) \\big) \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL in Neuroscience\n",
    "\n",
    "The most remarkable point of contact between RL and neuroscience involves dopamine. Dopamine appears to convey **TD errors** to brain structures where learning and dicision making take place. (**Reward prediction error hypothesis of depamine neuron activity** )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reward signals, Reinforcement signals, Values, and Prediction errors\n",
    "\n",
    "+ **Reward signal** : $R_t$, an abstraction summarizing the overall effect of a multitude of neural signals. \n",
    "\n",
    "+ **Reinforcement signal** : for a TD method, **TD error** : $\\delta_{t-1} = R_t + \\gamma V(S_t) - V(S_{t-1})$\n",
    "\n",
    "+ **Reward prediction error (RPE)** : descrepancies between the expected and the received reward signal. e.g. TD error\n",
    "\n",
    "Experimental evidence suggests that **dopamine signals RPEs**, and the phasic activity of dopamine conveys TD errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The reward prediction error hypothesis\n",
    "\n",
    "One of the functions of the **phasic** activity of dopamine-producing neurons in mammals is to deliver an **error between an old and a new estimate of expected future reward** to target areas throughout the brain.\n",
    "\n",
    "Montague, Dayan, and Sejnowski(1996) showed **how mesencephalic dopamine system could distribute to their targets a signal that represents information about future expectations**. They made several assumptions to set up the comparison between the **TD errors of the TD model of classical conditioning**(i.e. semi-gradient-descent TD($\\lambda$) algorithm with linear function approximation) and the **phasic activity of dopamine-producing neurons during classical conditioning**.\n",
    "\n",
    "+ Neurons cannot have a negative firing rate : thus, $\\delta_{t-1} + b_t$ where $b_t$ is the **background firing rate of the neuron**.\n",
    "\n",
    "+ **Complete serial compound**(CSC) representation, but the sequence of short-duration internal signals continues until the onset of the US (i.e. non-zero reward signal) : this allows the TD error to mimic the fact that dopamine neuron activity not only predicts a **future reward**, but that it is also sensitive to **when after a predictive cue that reward is expected to arrive**.\n",
    "\n",
    "TD errors parallel the following features of dopamine neuron activity.\n",
    "\n",
    "+ Phasic response of a dopamine neuron **only occurs when a rewarding event is unpredicted**.\n",
    "+ Continuous learning of neutral cues gain **predictive value** and come to elicit **phasic dopamine responses**.\n",
    "+ If an even earlier cue reliably precedes a cue that has already acquired predictive value, the phasic dopamine response **shifts to the earlier cue, ceasing for the later cue**.\n",
    "+ If the predicted rewarding event is omitted after learning, a dopamine neuron's response **decreases below its baseline level** shortly after the expected time of the rewarding event.\n",
    "\n",
    "Redish (2004) set a model based on the observation that administration of cocaine and some other addictive drugs produces **a transient increase in dopamine**. In the model, the **dopamine surge** is assumed to increase the TD error $\\delta$ in a way that **cannot be cancelled out by changes in the value function**. The model prevents $\\delta$ from becoming negative, thus eliminating the **error-correcting feature of TD learning** for states associated with administration of the drug. Thus, **the value increases without bound**, making actions leading to these states preferred above all others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Actor-Critic\n",
    "\n",
    "Two distinctive features of AC algorithms\n",
    "\n",
    "+ the actor and the critic components correspond to **dorsal** and **ventral** subdivisions of the striatum\n",
    "+ TD error has the dual role of **being the reinforcement signal for both** the actor and the critic\n",
    "\n",
    "Findings about the neural circuitry support the following : \n",
    "\n",
    "+ axons of dopamine neurons target both the **dorsal** and **ventral** subdivisions of the striatum\n",
    "+ dopamine appears to be critical for modulating synaptic plasticity in both structures\n",
    "+ how dopamine acts on a target depends on properties of the target structure and not just on properties of dopamine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and Critic Learning Rules\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_t &= R_{t+1} + \\gamma \\hat{v} (S_{t+1}, \\mathbf{w}) - \\hat{v} (S_{t+1}, \\mathbf{w}) \\\\[7pt]\n",
    "\\mathbf{z}_t^w &= \\lambda^w \\mathbf{z}_{t-1}^w + \\nabla \\hat{v} (S_t, \\mathbf{w}) \\\\[7pt]\n",
    "\\mathbf{z}_t^\\theta &= \\lambda^\\theta \\mathbf{z}_{t-1}^\\theta + \\nabla \\ln \\pi(A_t | S_t, \\theta) \\\\[7pt]\n",
    "\\mathbf{w} &:= \\mathbf{w} + \\alpha^w \\delta_t \\mathbf{z}_t^w \\\\[7pt]\n",
    "\\theta &:= \\theta + \\alpha^\\theta \\delta_t \\mathbf{z}_t^\\theta \\\\[7pt]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hedonistic Neuron Hypothesis\n",
    "\n",
    "Klopf(1972) conjectured that individual neurons seek to **maximize the difference** between synaptic input treated as **rewarding** and synaptic input treated as **punishing** by **adjusting the efficacies** of their synapses on the basis of rewarding or punishing consequences of their own APs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
