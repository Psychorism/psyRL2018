{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient (REINFORCE)\n",
    "\n",
    "## 1. Policy-based RL\n",
    "\n",
    "#### Value-based RL vs. Policy-based RL\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_\\theta (s) &\\approx V^\\pi (s) \\\\[10pt]\n",
    "Q_\\theta (s, a) &\\approx Q^\\pi (s, a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Direct parameterization of the policy\n",
    "\n",
    "$$\n",
    "\\pi_\\theta (s, a) = \\mathbb{P} [ a | s, \\theta ] \n",
    "$$\n",
    "\n",
    "+ choose the action **NOT based on the value function**, but based on **the features of the state**.\n",
    "+ i.e. **policy gradient** directly approximates the policy. \n",
    "\n",
    "#### Pros/Cons of policy-based RL\n",
    "\n",
    "**Pros**\n",
    "\n",
    "+ Better convergence properties\n",
    "+ Effective in high-dimensional/continuous action spaces\n",
    "+ Can learn **stochastic policies** (vs. action-value methods)\n",
    "\n",
    "**Cons**\n",
    "\n",
    "+ Typically, convergence to a local optimum\n",
    "+ Evaluation of a policy : typically inefficient & high variance\n",
    "\n",
    "#### When to adopt stochastic policies\n",
    "\n",
    "**Rock-Paper-Scissors**\n",
    "\n",
    "+ Any deterministic policy is easily exploited\n",
    "+ Optimal policy is discrete uniform random policy as a Nash equilibrium\n",
    "\n",
    "**Aliased Gridworld**\n",
    "\n",
    "+ Any feature might be incomplete : $\\phi(s,a) = \\mathbf{1}$ (Wall to the **North**, **AND** Action is to move the **East**\n",
    "+ Deterministic policy should choose one of the grey squares : no feature would differentiate the two greys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy gradient\n",
    "\n",
    "A method of **updating the approximated policy** according to the gradient ascent of the goal function $J(\\theta)$\n",
    "\n",
    "Seek to maximize performance, so their updates approximate gradient ascent \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{t+1} &= \\theta_t + \\alpha \\hat{\\nabla J(\\theta)} \\\\[10pt]\n",
    "&=  \\theta_t + \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\nabla_\\theta J(\\theta) = \\Biggl( \\begin{matrix}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_1} \\\\ \\vdots \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\end{matrix} \\Biggl)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective function $J$\n",
    "\n",
    "**Goal** : given policy $\\pi_\\theta (s, a)$, **find best $\\theta$ **\n",
    "\n",
    "How to measure the quality of a policy $\\pi_\\theta$? : **policy gradient applies equally**\n",
    "\n",
    "+ **start value** : episodic environments $\\big( J_1(\\theta) = V^{\\pi_\\theta} (s1) = \\mathbb{E} _{\\pi_\\theta} [v_1] \\big) $\n",
    "\n",
    "+ **average value** : continuous environments $\\big( J_{avV} (\\theta) = \\sum_s \\mu^{\\pi_\\theta} (s) V^{\\pi_\\theta} (s) \\big)$\n",
    "\n",
    "+ **average reward per time-step** : $J_{avR} (\\theta) = \\sum_s \\mu^{\\pi_\\theta} (s) \\sum_a \\pi_\\theta (s,a) R^a_s$\n",
    "\n",
    "where $\\mu^{\\pi_\\theta} (s)$ : **stationary distribution** of MDP $\\pi_\\theta$, i.e. the probability of agent being at state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Function\n",
    "\n",
    "**Likelihood ratios** : exploit the following identity\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta \\pi_\\theta (s,a) &= \\pi_\\theta (s,a) \\frac{\\nabla_\\theta \\pi_\\theta (s,a)} {\\pi_\\theta (s,a)} \\\\[10pt]\n",
    "&= \\pi_\\theta (s,a) \\nabla_\\theta \\log \\pi_\\theta (s, a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\nabla_\\theta \\log \\pi_\\theta (s, a)$ : **score function**\n",
    "\n",
    "\n",
    "#### Softmax Policy\n",
    "\n",
    "Unlike Deep SARSA/DQN, the **ouput is the probability for the actions.** Thus, it uses **softmax** function.\n",
    "\n",
    "$$\n",
    "\\sigma(y_i) = \\frac{e^{y_i}}{\\sum_j e^{y_j}}\n",
    "$$\n",
    "\n",
    "+ **Weighted actions** using linear combination of features $\\phi(s,a)^T \\theta$\n",
    "+ Probability of action is proportional to **exponentiated weight**\n",
    "\n",
    "$$\n",
    "\\pi_\\theta (s,a) \\propto e^{\\phi(s,a)^T \\theta}\n",
    "$$\n",
    "\n",
    "#### Gaussian Policy\n",
    "\n",
    "+ **Policy is Gaussian distributed** i.e. $a \\sim N(\\mu(s), \\sigma^2)$\n",
    "+ **Mean** : a linear combination of state features $\\mu (s) = \\phi (s)^T \\theta$\n",
    "+ **Variance** : fixed/parameterized\n",
    "+ **Score function**\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\log \\pi_\\theta (s,a) = \\frac{(a - \\mu(s)) \\phi(s)} {\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_{\\theta} \\mathbf{v}_{\\pi_\\theta} (s_0) \\\\[15pt]\n",
    "&= \\sum_s d_{\\pi_\\theta}(s) \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\Big[ \\nabla_\\theta \\log \\pi_\\theta (a \\mid s) q_\\pi (s, a) \\Big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_{t+1} \\equiv w_t + \\alpha \\Big( R_{t+1} + \\gamma \\mathbf{w}_t^T \\mathbf{x}-(t+1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By perturbing $\\theta$ by small amount $\\varepsilon$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k} \\approx \\frac{J(\\theta + \\varepsilon u_k) - J(\\theta)} {\\varepsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Gradient Theorem\n",
    "\n",
    "**Policy gradient theorem** generalises the **likelihood ratio** approach to multi-step MDPs\n",
    "\n",
    "It provides a way to calculate $J(\\theta)$ w.r.t. **policy parameters**, with **$Q$-function**, even if we do not know \n",
    "\n",
    "+ **True value function** $v_{\\pi_\\theta}$\n",
    "+ **State distribution** (i.e. distribution of rewards)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla J(\\theta) &\\propto \\sum_s \\mu_\\pi(s) \\sum_a q_\\pi (s,a) \\nabla \\pi (a | s, \\theta) \\\\[10pt]\n",
    "&= \\mathbb{E}_\\pi \\Big[ \\sum_a \\nabla \\pi_\\theta (a|s) q^{\\pi_\\theta} (s,a) \\Big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mu_\\pi(s)$ : on-policy state distribution under $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof of Policy Gradient Theorem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla v_\\pi (s) &= \\nabla \\Big[ \\sum_a \\pi (a | s) q_\\pi (s, a) \\Big], \\forall s \\in \\mathfrak{S} \\\\[10pt]\n",
    "&= \\sum_a \\Big[ \\nabla \\pi (a | s) q_\\pi (s, a) + \\pi (a | s) \\nabla q_\\pi (s,a) \\Big] (\\because \\ \\ \\text{product rule & Leibniz rule)} \\\\[10pt]\n",
    "&= \\sum_a \\Big[ \\nabla \\pi (a | s) q_\\pi (s, a) + \\pi (a | s) \\nabla \\sum_{s', r} p(s', r |s, a)(r + v_\\pi (s') ) \\Big] \\\\[10pt]\n",
    "&= \\sum_a \\Big[ \\nabla \\pi (a | s) q_\\pi (s, a) + \\pi (a | s) \\sum_{s'} p(s'|s, a) \\nabla v_\\pi (s')  \\Big] \\\\[10pt]\n",
    "&= \\sum_{x \\in S} \\sum_{k=0}^\\infty P(s \\rightarrow x, k, \\pi) \\sum_a \\nabla \\pi (a | x) q_\\pi (x, a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\sum_{x \\in S} \\sum_{k=0}^\\infty P(s \\rightarrow x, k, \\pi)$ : probability of transitioning from state $s$ to $x$ in $k$ steps under $\\pi$, and the above is clear from the repetitive substitution, using the following fact.\n",
    "\n",
    "$$\n",
    "\\nabla v_\\pi (s') = \\sum_{a'} \\Big[ \\nabla \\pi (a' | s') q_\\pi (s', a') + \\pi (a' | s') \\sum_{s''} p(s''|s', a') \\nabla v_\\pi (s'')  \\Big]\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla J(\\theta) &= \\nabla v_\\pi (s_0) \\\\[10pt]\n",
    "&= \\sum_s \\eta(s) \\sum_a \\nabla \\pi (a | x) q_\\pi (x, a) \\ \\ \\big(\\because \\ \\ \\eta(s) = \\sum_{k=0}^\\infty P(s_0 \\rightarrow s, k, \\pi)\\big) \\\\[10pt]\n",
    "&\\propto \\sum_a \\mu(s) \\sum_s \\eta(s) \\sum_a \\nabla \\pi (a | x) q_\\pi (x, a) \\ \\ \\big(\\because \\ \\ \\eta(s') \\propto \\mu(s)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\eta(s)$ : number of time steps spent on average, in state $s$ in a single episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. REINFORCE : Monte Carlo Policy Gradient\n",
    "\n",
    "**Model-free** RL\n",
    "\n",
    "**Stochastic gradient ascent** : requires a way to obtain samples such that the expectation of the sample gradient is proportional to the actual gradient of the performance measure as a function of the parameter.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla J(\\theta) &=\\sum_s \\mu_\\pi (s) \\sum_s \\nabla \\pi (a | s) q_\\pi (s, a) \\\\[10pt]\n",
    "&=\\sum_s \\mu_\\pi (s) \\sum_s \\pi(a |s) \\frac{\\nabla \\pi (a | s)}{\\pi(a |s)} q_\\pi (s, a) \\\\[10pt]\n",
    "&=\\sum_s \\mu_\\pi (s) \\sum_s \\pi(a |s) \\big[ \\nabla \\log \\pi (a | s) q_\\pi (s, a) \\big] \\\\[10pt]\n",
    "&= \\mathbb{E}_\\pi \\big[ \\nabla \\log \\pi (a | s) q_\\pi (s, a) \\big]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating Formula of REINFORCE\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{t + 1} &\\approx \\theta_t + \\alpha \\big[ \\nabla_\\theta \\log \\pi_\\theta (a | s) G_t \\big]  (*) \\\\[10pt]\n",
    "&= \\theta_t - \\alpha \\big[ \\nabla_\\theta \\big(- \\log \\pi_\\theta (a | s) \\big) G_t \\big]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE with Baseline\n",
    "\n",
    "In order to reduce variance (and thus the speed of learning), we **use the following**. The baseline does not depend on $a$.\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a \\Big( q_\\pi (s,a) - b(s) \\Big) \\nabla \\pi(a | s, \\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} := \\theta_t + \\alpha \\big( G_t - b(S_t) \\big) \\frac{\\nabla \\pi (A_t | S_t, \\theta_t) } {\\pi (A_t | S_t, \\theta_t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithm\n",
    "\n",
    "#### Overview\n",
    "\n",
    "**1. Input** : a **differentiable** policy parameterization $\\pi (a | s, \\theta), \\ \\ \\forall a \\in A, s \\in S, \\theta \\in \\mathbb{R}^n$\n",
    "\n",
    "**2. Initialize policy weights** $\\theta$\n",
    "\n",
    "**3. Repeat**\n",
    "\n",
    "3.1. Generate an episode : $S_0, A_0, R_1, \\cdots, S_{T-1}, A_{T-1}, R_T$, following $\\pi(\\cdot | \\cdot, \\theta)$\n",
    "\n",
    "3.2. For each step of the episode : $t=0, \\cdots T-1$ :\n",
    "\n",
    "    G_t <- return from step t\n",
    "    theta <- theta + alpha gamma^t G_t del log pi (A | S, theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose an action using the policy network\n",
    "\n",
    "Not need $\\varepsilon$-greedy, since the **policy itself is stochastic**.\n",
    "\n",
    "```python\n",
    "    def get_action(self, state):\n",
    "            policy = self.model.predict(state)[0]\n",
    "            return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the return\n",
    "\n",
    "Update $\\theta$ using $\\theta_{t + 1} \\approx \\theta_t + \\alpha \\big[ \\nabla_\\theta \\log \\pi_\\theta (a | s) G_t \\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state[0])\n",
    "        self.rewards.append(reward)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the loss function & training function to update policy network\n",
    "\n",
    "```python\n",
    "    def optimizer(self):\n",
    "        action = K.placeholder(shape=[None, 5])\n",
    "        discounted_rewards = K.placeholder(shape=[None, ])\n",
    "        \n",
    "        # calculate the cross-entropy loss function\n",
    "        action_prob = K.sum(action * self.model.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * discounted_rewards\n",
    "        loss = -K.sum(cross_entropy)\n",
    "        \n",
    "        # create a training function to update policy network\n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights,[],\n",
    "                                        loss)\n",
    "        train = K.function([self.model.input, action, discounted_rewards], [],\n",
    "                           updates=updates)\n",
    "\n",
    "        return train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Loss function as the goal of policy network updating**\n",
    "\n",
    "Since \n",
    "\n",
    "$$\n",
    "\\big[ \\nabla_\\theta \\log \\pi_\\theta (a | s) \\big] G_t = \\nabla_\\theta \\big[ log \\pi_\\theta (a | s) G_t \\big]\n",
    "$$\n",
    "\n",
    "so the **goal of policy network updating** (i.e. **loss function**) is $\\log \\pi_\\theta (a | s) G_t$\n",
    "\n",
    "**2. The meaning of the loss function : cross-entropy**\n",
    "\n",
    "$$\n",
    "H(p,y) = - \\sum_i y_i \\log p_i\n",
    "$$\n",
    "\n",
    "measures **how similar** between the ground truth $y$ (**the selected action**) and the preditionc $p$ (the **policy**)\n",
    "\n",
    "**3. Gradient Ascent**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{t + 1} &\\approx \\theta_t + \\alpha \\big[ \\nabla_\\theta \\log \\pi_\\theta (a | s) G_t \\big] \\\\[10pt]\n",
    "&= \\theta_t - \\alpha \\big[ \\nabla_\\theta \\big(- \\log \\pi_\\theta (a | s) \\big) G_t \\big]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the policy network\n",
    "\n",
    "```python\n",
    "    def train_model(self):\n",
    "        discounted_rewards = np.float32(self.discount_rewards(self.rewards))\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        self.optimizer([self.states, self.actions, discounted_rewards])\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra) (Shanon's) Entropy\n",
    "\n",
    "the information required to specify a system's state\n",
    "\n",
    "**Entropy** is defined as follows\n",
    "\n",
    "$$\n",
    "\\mathfrak{E} (\\pi) = - \\sum_\\Theta \\pi(\\theta_i) \\log \\pi (\\theta_i)\n",
    "$$\n",
    "\n",
    "where $\\pi$ : pdf on $\\Theta$, which is discrete. \n",
    "\n",
    "**Different** from **standard deviation**, as in the case of a bimodal distribution.\n",
    "\n",
    "**Maximum entropy** : probability distribution when $\\pi(\\theta_i) = 1/n, \\ \\ \\forall i$\n",
    "\n",
    "$$\n",
    "\\mathfrak{E} (\\pi) = - \\sum_{i=1}^n \\frac{1}{n} \\log(\\frac{1}{n}) = \\log n\n",
    "$$\n",
    "\n",
    "(**noninformative prior** for a discrete $\\Theta$)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
